{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/diane.kim/ACS/tabular/model/TabPFN/src')\n",
    "sys.path.append('/home/diane.kim/nature/model/src')\n",
    "\n",
    "from tabpfn.classifier import TabPFNClassifier\n",
    "from tabpfn.config import ModelInterfaceConfig\n",
    "from tabpfn.preprocessing import PreprocessorConfig\n",
    "\n",
    "from utils.data_preprocessor import *\n",
    "from utils.fix_seed import *\n",
    "from models import main, parse_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diane.kim/nature/model/utils/data_preprocessor.py:281: DtypeWarning: Columns (94) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ref_df = pd.read_csv(ref_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5851, 40)\n",
      "Running MANAG vs INTERVENTION.\n",
      "MANAG\n",
      "INTERVENTION    4757\n",
      "MANAG           1094\n",
      "Name: count, dtype: int64\n",
      "Label Mapping Used:\n",
      "MANAG: 0\n",
      "INTERVENTION: 1\n",
      "ROC AUC: 0.7242\n",
      "Accuracy: 0.7063\n",
      "Balanced Accuracy: 0.6541\n",
      "F1 Score: 0.7456\n",
      "Recall: 0.7063\n",
      "Precision: 0.8203\n",
      "Class Accuracy:\n",
      "  0: 0.5811\n",
      "  1: 0.7271\n",
      "Spec 20% → Sens: 0.9553, Spec: 0.2027, Thr: 0.3118\n",
      "Spec 40% → Sens: 0.8949, Spec: 0.4054, Thr: 0.3712\n",
      "Spec 60% → Sens: 0.7114, Spec: 0.6216, Thr: 0.5274\n",
      "\n",
      "(4569, 192)\n",
      "emb_train tensor shape:  torch.Size([4569, 192])\n"
     ]
    }
   ],
   "source": [
    "# seed=0\n",
    "\n",
    "# X_train, X_val, X_test, y_train, y_val, y_test, train_ids, val_ids, test_ids, preprocessor = load_data(binary=True, seed=seed, ref_df=\"/home/diane.kim/nature/data/final/MASTER_2262_wTTE.csv\")\n",
    "\n",
    "# pfn = init_tabpfn(seed=seed, balance_probabilities=True)\n",
    "# pfn = train_tabpfn(pfn, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# emb_train, emb_val, emb_test = get_tab_embeddings(pfn, X_train, X_val, X_test)\n",
    "\n",
    "# # X = emb_train.cpu().numpy()\n",
    "# # y = y_train.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameter grid to try\n",
    "# n_neighbors_list = [5, 15, 50]\n",
    "# min_dist_list    = [0.0, 0.1, 0.5]\n",
    "# metric = \"cosine\"          # \"euclidean\" also fine; \"cosine\" often good for embeddings\n",
    "# random_state = 42\n",
    "\n",
    "# # Make sure output dir exists\n",
    "# os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "# # Create the grid of subplots\n",
    "# n_rows = len(min_dist_list)\n",
    "# n_cols = len(n_neighbors_list)\n",
    "# fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows), squeeze=False)\n",
    "\n",
    "# for i, md in enumerate(min_dist_list):\n",
    "#     for j, nn in enumerate(n_neighbors_list):\n",
    "#         ax = axes[i, j]\n",
    "#         reducer = umap.UMAP(\n",
    "#             n_neighbors=nn,\n",
    "#             min_dist=md,\n",
    "#             metric=metric,\n",
    "#             n_components=2,\n",
    "#             random_state=random_state\n",
    "#         )\n",
    "#         X_umap = reducer.fit_transform(X)  # [N, 2]\n",
    "\n",
    "#         sc = ax.scatter(\n",
    "#             X_umap[:, 0], X_umap[:, 1],\n",
    "#             c=y, cmap=\"coolwarm\", s=6, alpha=0.7, linewidths=0\n",
    "#         )\n",
    "#         ax.set_title(f\"n_neighbors={nn}, min_dist={md}\", fontsize=11)\n",
    "#         ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "# # One shared colorbar for the whole grid\n",
    "# cbar = fig.colorbar(sc, ax=axes, shrink=0.7, pad=0.01)\n",
    "# cbar.set_label(\"Label (0=MANAG, 1=INTERVENTION)\")\n",
    "\n",
    "# fig.suptitle(\"UMAP projection of TabPFN embeddings (grid of settings)\", y=0.995, fontsize=14)\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "# out_path = \"plots/umap_tabpfn_grid.png\"\n",
    "# plt.savefig(out_path, dpi=300)\n",
    "# plt.close()\n",
    "# print(f\"Saved grid figure to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diane.kim/miniconda3/envs/term2/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/diane.kim/miniconda3/envs/term2/lib/python3.12/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# reducer = umap.UMAP(\n",
    "#     n_neighbors=5,\n",
    "#     min_dist=0.5,\n",
    "#     metric=\"cosine\",\n",
    "#     n_components=3,\n",
    "#     random_state=seed\n",
    "# )\n",
    "\n",
    "# X_umap = reducer.fit_transform(X)  # [N, 3]\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 6))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# sc = ax.scatter(\n",
    "#     X_umap[:, 0], X_umap[:, 1], X_umap[:, 2],\n",
    "#     c=y, cmap=\"coolwarm\", s=6, alpha=0.7, linewidths=0\n",
    "# )\n",
    "\n",
    "# ax.set_title(f\"n_neighbors={5}, min_dist={0.5}\", fontsize=11)\n",
    "# ax.set_xlabel(\"UMAP-1\"); ax.set_ylabel(\"UMAP-2\"); ax.set_zlabel(\"UMAP-3\")\n",
    "# fig.colorbar(sc, ax=ax, shrink=0.75, pad=0.05, label=\"Label (0=MANAG, 1=INTERVENTION)\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"umap_tabpfn_embeddings_3d.png\", dpi=300)\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diane.kim/nature/model/utils/data_preprocessor.py:281: DtypeWarning: Columns (94) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ref_df = pd.read_csv(ref_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5851, 40)\n",
      "Running MANAG vs INTERVENTION.\n",
      "MANAG\n",
      "INTERVENTION    4757\n",
      "MANAG           1094\n",
      "Name: count, dtype: int64\n",
      "Label Mapping Used:\n",
      "MANAG: 0\n",
      "INTERVENTION: 1\n",
      "ROC AUC: 0.7242\n",
      "Accuracy: 0.7063\n",
      "Balanced Accuracy: 0.6541\n",
      "F1 Score: 0.7456\n",
      "Recall: 0.7063\n",
      "Precision: 0.8203\n",
      "Class Accuracy:\n",
      "  0: 0.5811\n",
      "  1: 0.7271\n",
      "Spec 20% → Sens: 0.9553, Spec: 0.2027, Thr: 0.3118\n",
      "Spec 40% → Sens: 0.8949, Spec: 0.4054, Thr: 0.3712\n",
      "Spec 60% → Sens: 0.7114, Spec: 0.6216, Thr: 0.5274\n",
      "\n",
      "(4569, 192)\n",
      "emb_train tensor shape:  torch.Size([4569, 192])\n",
      "Epoch 000 | train loss 0.251 | val loss 0.255 | val accuracy 0.604\n",
      "Epoch 001 | train loss 0.241 | val loss 0.260 | val accuracy 0.648\n",
      "Epoch 002 | train loss 0.239 | val loss 0.261 | val accuracy 0.469\n",
      "Epoch 003 | train loss 0.239 | val loss 0.257 | val accuracy 0.604\n",
      "Epoch 004 | train loss 0.240 | val loss 0.264 | val accuracy 0.424\n",
      "Epoch 005 | train loss 0.241 | val loss 0.256 | val accuracy 0.582\n",
      "Epoch 006 | train loss 0.240 | val loss 0.262 | val accuracy 0.512\n",
      "Epoch 007 | train loss 0.239 | val loss 0.264 | val accuracy 0.535\n",
      "Epoch 008 | train loss 0.240 | val loss 0.256 | val accuracy 0.580\n",
      "Epoch 009 | train loss 0.240 | val loss 0.262 | val accuracy 0.624\n",
      "Epoch 010 | train loss 0.239 | val loss 0.262 | val accuracy 0.601\n",
      "Epoch 011 | train loss 0.241 | val loss 0.264 | val accuracy 0.627\n",
      "Epoch 012 | train loss 0.241 | val loss 0.259 | val accuracy 0.615\n",
      "Epoch 013 | train loss 0.240 | val loss 0.257 | val accuracy 0.607\n",
      "Epoch 014 | train loss 0.241 | val loss 0.256 | val accuracy 0.602\n",
      "Epoch 015 | train loss 0.241 | val loss 0.264 | val accuracy 0.603\n",
      "Epoch 016 | train loss 0.242 | val loss 0.257 | val accuracy 0.610\n",
      "Epoch 017 | train loss 0.241 | val loss 0.259 | val accuracy 0.628\n",
      "Epoch 018 | train loss 0.242 | val loss 0.264 | val accuracy 0.683\n",
      "Epoch 019 | train loss 0.240 | val loss 0.257 | val accuracy 0.578\n",
      "Epoch 020 | train loss 0.242 | val loss 0.255 | val accuracy 0.601\n",
      "Epoch 021 | train loss 0.241 | val loss 0.263 | val accuracy 0.724\n",
      "Epoch 022 | train loss 0.241 | val loss 0.257 | val accuracy 0.560\n",
      "Epoch 023 | train loss 0.242 | val loss 0.259 | val accuracy 0.627\n",
      "Epoch 024 | train loss 0.239 | val loss 0.258 | val accuracy 0.612\n",
      "Epoch 025 | train loss 0.241 | val loss 0.255 | val accuracy 0.562\n",
      "Epoch 026 | train loss 0.241 | val loss 0.278 | val accuracy 0.401\n",
      "Epoch 027 | train loss 0.241 | val loss 0.258 | val accuracy 0.612\n",
      "Epoch 028 | train loss 0.243 | val loss 0.258 | val accuracy 0.601\n",
      "Epoch 029 | train loss 0.240 | val loss 0.258 | val accuracy 0.604\n",
      "Epoch 030 | train loss 0.241 | val loss 0.257 | val accuracy 0.593\n",
      "Epoch 031 | train loss 0.241 | val loss 0.259 | val accuracy 0.608\n",
      "Epoch 032 | train loss 0.241 | val loss 0.255 | val accuracy 0.414\n",
      "Epoch 033 | train loss 0.242 | val loss 0.258 | val accuracy 0.589\n",
      "Epoch 034 | train loss 0.241 | val loss 0.258 | val accuracy 0.580\n",
      "Epoch 035 | train loss 0.241 | val loss 0.263 | val accuracy 0.625\n",
      "Epoch 036 | train loss 0.240 | val loss 0.265 | val accuracy 0.394\n",
      "Epoch 037 | train loss 0.239 | val loss 0.259 | val accuracy 0.595\n",
      "Epoch 038 | train loss 0.252 | val loss 0.265 | val accuracy 0.573\n",
      "Epoch 039 | train loss 0.249 | val loss 0.261 | val accuracy 0.608\n",
      "Epoch 040 | train loss 0.243 | val loss 0.259 | val accuracy 0.606\n",
      "Epoch 041 | train loss 0.242 | val loss 0.258 | val accuracy 0.581\n",
      "Epoch 042 | train loss 0.241 | val loss 0.261 | val accuracy 0.602\n",
      "Epoch 043 | train loss 0.239 | val loss 0.268 | val accuracy 0.607\n",
      "Epoch 044 | train loss 0.241 | val loss 0.264 | val accuracy 0.435\n",
      "Epoch 045 | train loss 0.240 | val loss 0.256 | val accuracy 0.537\n",
      "Epoch 046 | train loss 0.239 | val loss 0.263 | val accuracy 0.604\n",
      "Epoch 047 | train loss 0.241 | val loss 0.257 | val accuracy 0.607\n",
      "Epoch 048 | train loss 0.238 | val loss 0.266 | val accuracy 0.580\n",
      "Epoch 049 | train loss 0.241 | val loss 0.275 | val accuracy 0.608\n",
      "Epoch 050 | train loss 0.240 | val loss 0.265 | val accuracy 0.607\n",
      "Epoch 051 | train loss 0.250 | val loss 0.267 | val accuracy 0.423\n",
      "Epoch 052 | train loss 0.251 | val loss 0.266 | val accuracy 0.625\n",
      "Epoch 053 | train loss 0.250 | val loss 0.267 | val accuracy 0.602\n",
      "Epoch 054 | train loss 0.251 | val loss 0.266 | val accuracy 0.608\n",
      "Epoch 055 | train loss 0.250 | val loss 0.265 | val accuracy 0.602\n",
      "Epoch 056 | train loss 0.251 | val loss 0.268 | val accuracy 0.568\n",
      "Epoch 057 | train loss 0.250 | val loss 0.267 | val accuracy 0.591\n",
      "Epoch 058 | train loss 0.251 | val loss 0.268 | val accuracy 0.423\n",
      "Epoch 059 | train loss 0.251 | val loss 0.268 | val accuracy 0.516\n",
      "Epoch 060 | train loss 0.250 | val loss 0.273 | val accuracy 0.541\n",
      "Epoch 061 | train loss 0.251 | val loss 0.280 | val accuracy 0.493\n",
      "Epoch 062 | train loss 0.251 | val loss 0.265 | val accuracy 0.603\n",
      "Epoch 063 | train loss 0.250 | val loss 0.266 | val accuracy 0.598\n",
      "Epoch 064 | train loss 0.250 | val loss 0.268 | val accuracy 0.664\n",
      "Epoch 065 | train loss 0.251 | val loss 0.267 | val accuracy 0.574\n",
      "Epoch 066 | train loss 0.250 | val loss 0.272 | val accuracy 0.537\n",
      "Epoch 067 | train loss 0.252 | val loss 0.268 | val accuracy 0.543\n",
      "Epoch 068 | train loss 0.249 | val loss 0.266 | val accuracy 0.608\n",
      "Epoch 069 | train loss 0.250 | val loss 0.268 | val accuracy 0.583\n",
      "Epoch 070 | train loss 0.250 | val loss 0.270 | val accuracy 0.580\n",
      "Epoch 071 | train loss 0.249 | val loss 0.266 | val accuracy 0.633\n",
      "Epoch 072 | train loss 0.250 | val loss 0.268 | val accuracy 0.580\n",
      "Epoch 073 | train loss 0.250 | val loss 0.267 | val accuracy 0.598\n",
      "Epoch 074 | train loss 0.250 | val loss 0.274 | val accuracy 0.431\n",
      "Epoch 075 | train loss 0.249 | val loss 0.268 | val accuracy 0.601\n",
      "Epoch 076 | train loss 0.249 | val loss 0.269 | val accuracy 0.590\n",
      "Epoch 077 | train loss 0.249 | val loss 0.271 | val accuracy 0.582\n",
      "Epoch 078 | train loss 0.249 | val loss 0.272 | val accuracy 0.568\n",
      "Epoch 079 | train loss 0.250 | val loss 0.273 | val accuracy 0.505\n",
      "Epoch 080 | train loss 0.249 | val loss 0.267 | val accuracy 0.589\n",
      "Epoch 081 | train loss 0.249 | val loss 0.278 | val accuracy 0.534\n",
      "Epoch 082 | train loss 0.249 | val loss 0.266 | val accuracy 0.573\n",
      "Epoch 083 | train loss 0.249 | val loss 0.268 | val accuracy 0.595\n",
      "Epoch 084 | train loss 0.249 | val loss 0.267 | val accuracy 0.590\n",
      "Epoch 085 | train loss 0.248 | val loss 0.269 | val accuracy 0.599\n",
      "Epoch 086 | train loss 0.248 | val loss 0.272 | val accuracy 0.576\n",
      "Epoch 087 | train loss 0.250 | val loss 0.271 | val accuracy 0.561\n",
      "Epoch 088 | train loss 0.248 | val loss 0.271 | val accuracy 0.552\n",
      "Epoch 089 | train loss 0.249 | val loss 0.270 | val accuracy 0.564\n",
      "Epoch 090 | train loss 0.248 | val loss 0.270 | val accuracy 0.574\n",
      "Epoch 091 | train loss 0.248 | val loss 0.269 | val accuracy 0.553\n",
      "Epoch 092 | train loss 0.248 | val loss 0.269 | val accuracy 0.560\n",
      "Epoch 093 | train loss 0.248 | val loss 0.269 | val accuracy 0.560\n",
      "Epoch 094 | train loss 0.248 | val loss 0.269 | val accuracy 0.569\n",
      "Epoch 095 | train loss 0.248 | val loss 0.269 | val accuracy 0.557\n",
      "Epoch 096 | train loss 0.248 | val loss 0.269 | val accuracy 0.564\n",
      "Epoch 097 | train loss 0.248 | val loss 0.269 | val accuracy 0.558\n",
      "Epoch 098 | train loss 0.248 | val loss 0.269 | val accuracy 0.557\n",
      "Epoch 099 | train loss 0.248 | val loss 0.269 | val accuracy 0.558\n",
      "Final epoch model weights saved for seed 0.\n",
      "[TEST]\n",
      "ROC AUC: 0.7067\n",
      "Accuracy: 0.6718\n",
      "Balanced Accuracy: 0.6734\n",
      "F1 Score: 0.7201\n",
      "Recall: 0.6718\n",
      "Precision: 0.8305\n",
      "Class Accuracy:\n",
      "  0: 0.6757\n",
      "  1: 0.6711\n",
      "Spec 20% → Sens: 0.9575, Spec: 0.2162, Thr: 0.2538\n",
      "Spec 40% → Sens: 0.8702, Spec: 0.4054, Thr: 0.3782\n",
      "Spec 60% → Sens: 0.7315, Spec: 0.6081, Thr: 0.4915\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diane.kim/miniconda3/envs/term2/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/diane.kim/miniconda3/envs/term2/lib/python3.12/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "seed=0\n",
    "\n",
    "def collect_hidden_activations(model, X, y, device, batch_size=32, relu_index=1):\n",
    "    model.eval()\n",
    "    activation_tensors = []\n",
    "    labels = []\n",
    "\n",
    "    handle = model[relu_index].register_forward_hook(\n",
    "        lambda layer, input, output: activation_tensors.append(output.detach().cpu())\n",
    "    )\n",
    "    train_loader = DataLoader(TensorDataset(emb_train, y_train), batch_size=batch_size, sampler=None, shuffle=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in train_loader:\n",
    "            _ = model(x.to(device))\n",
    "            labels.append(y.cpu())\n",
    "    handle.remove()\n",
    "    activation = torch.cat(activation_tensors, dim=0)\n",
    "    Y = torch.cat(labels, dim=0)\n",
    "    return activation.numpy(), Y.numpy()\n",
    "\n",
    "def umap_and_plot(X_np, y_np, out_png, n_components=3,\n",
    "                  n_neighbors=5, min_dist=0.5, metric=\"cosine\", seed=seed, title=\"\"):\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        metric=metric,\n",
    "        n_components=n_components,\n",
    "        random_state=seed\n",
    "    )\n",
    "    X_umap = reducer.fit_transform(X_np)\n",
    "\n",
    "    if n_components == 2:\n",
    "        plt.figure(figsize=(8,6))\n",
    "        sc = plt.scatter(X_umap[:,0], X_umap[:,1], c=y_np, cmap=\"coolwarm\", s=6, alpha=0.7)\n",
    "        plt.colorbar(sc, label=\"Label (0=MANAG, 1=INTERVENTION)\")\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"UMAP-1\"); plt.ylabel(\"UMAP-2\")\n",
    "        plt.tight_layout(); plt.savefig(out_png, dpi=300); plt.close()\n",
    "    else:\n",
    "        from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "        fig = plt.figure(figsize=(8,6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        sc = ax.scatter(X_umap[:,0], X_umap[:,1], X_umap[:,2], c=y_np, cmap=\"coolwarm\", s=4, alpha=0.8)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(\"UMAP-1\"); ax.set_ylabel(\"UMAP-2\"); ax.set_zlabel(\"UMAP-3\")\n",
    "        fig.colorbar(sc, ax=ax, shrink=0.75, pad=0.05, label=\"Label (0=MANAG, 1=INTERVENTION)\")\n",
    "        plt.tight_layout(); plt.savefig(out_png, dpi=300); plt.close()\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, train_ids, val_ids, test_ids, preprocessor = load_data(binary=True, seed=seed, ref_df=\"/home/diane.kim/nature/data/final/MASTER_2262_wTTE.csv\")\n",
    "model, emb_train = train_model(X_train, y_train, X_val, y_val, X_test, y_test, num_epochs=100, optimizer_=\"SGD\", seed=seed, device=\"cuda\")\n",
    "activation_train, y_train_np = collect_hidden_activations(model, emb_train, y_train, device=\"cuda\")\n",
    "\n",
    "# umap_and_plot(\n",
    "#     activation_train, y_train_np,\n",
    "#     out_png=\"plots/umap_mlp_train_3d.png\",\n",
    "#     title=\"UMAP (3D) of MLP hidden activations — train\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diane.kim/miniconda3/envs/term2/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/diane.kim/miniconda3/envs/term2/lib/python3.12/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "umap_and_plot(\n",
    "    activation_train, y_train_np,\n",
    "    n_neighbors=3, min_dist=0.8,\n",
    "    out_png=\"plots/umap_mlp_train_3d.png\",\n",
    "    title=\"UMAP (3D) of MLP hidden activations — train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "term2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
